1. What should you do if the two models have different tokenizers?
If the tokenizers are different, it will be more difficult to calculate the log probabilities because we need both the expert and amateur probability of the same token. Use the expert tokenizer on the input to get the expert's  high probability tokens. Each expert token would be decoded, then re-tokenized using the amateur tokenizer. If it turned into just one token, then just use that probability. If it turned into multiple tokens, calculate the probability using the equation in section 2 of the contrastive decoding paper. This resulting probability will be the amateur probability of the expert's token.

2. Do you think contrastive decoding is used in practice?
Probably not, it seems to only be suited for specific situations. It relies on having one model which is trusted to be better than another model at the task, so it may over-emphasize mistakes made by the expert model. It also involves a significant increase in computation, so it might be better to invest more in improving only the expert. On the other hand if amateur and a trusted expert model are available and compute is less of a concern, it does seem effective at the goal of reducing redundancy.